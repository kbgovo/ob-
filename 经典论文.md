---
share: true
---

# [GCN](<file:///C:\Users\DELL\Desktop\论文\GCN.pdf>)
## 网络架构
![[GCN网络架构.png|GCN网络架构.png]]
## 具体实现
网络逐层传播规则：
$$
H^{(l+1)}=\sigma\Bigl(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}\Bigr)\ 
$$
其中$\tilde{A}=A+I$,$A$表示邻接矩阵，$D$是度矩阵，$H^{(l)}$是节点特征矩阵，$W$是可学习参数矩阵，$\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$的作用是对称归一化矩阵。
交叉熵损失函数为：
$$
{\mathcal{L}}=-\sum_{l\in{\mathcal{V}}_{L}}\sum_{f=1}^{F}Y_{l f}\ln Z_{l f}\,,
$$
实验中的前向模型：
$$
Z=f(X,A)=\mathrm{softmax}\bigg(\hat{A}\mathrm{~ReL}\mathrm{U}\Big(\hat{A}X W^{(0)}\Big)\,W^{(1)}\bigg)\ 
$$
其中$\hat{A}=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$
# [GAT](<file:///C:\Users\DELL\Desktop\论文\GRAPH ATTENTION NETWORKS.pdf>)：GRAPH ATTENTION NETWORKS

## 网络示意图
![[GAT网络示意.png|GAT网络示意.png]]
## 具体实现
计算注意力相关系数(表示节点 $j$ 对节点 $i$ 的重要性)：
$$
e_{i j}=\,a\big(W \vec{h}_{i}\,,\,W\vec{h}_{j}\big)
$$
其中$W$是共享权重矩阵，$\vec{h}_{i}$是节点特征向量，$j$ 是 $i$ 的一阶邻居。  

为了使节点间的系数易于比较，需要标准化$e_{i j}$ ：
$$
\alpha_{ij}=\operatorname{softmax}_j(e_{ij})={\frac{\exp(e_{ij})}{\sum_{k\in{{N}}_i}\exp(e_{ik})}}.
$$
完整的注意力系数计算公式：
$$
\alpha_{ij}=\frac{\exp\left(\mathrm{LeakyReLU}\left(\vec{\bf a}^{T}[{W}\vec{h}_{i}||{W}\vec{h}_{j}]\right)\right)}{{}\sum_{k\in
N_i}\exp\left(\mathrm{LeakyReLU}\left(\vec{\bf a}^{T}[{W}\vec{h}_{i}||{W}\vec{h}_{j}]\right)\right)}
$$
其中 $||$ 表示串联两个向量。

多头注意力机制：
$$
\vec{h}_i^{\prime}=\sigma\left(\frac{1}{K}\sum_{k=1}^K\sum_{j\in{N}_i}\alpha_{ij}^kW^k\vec{h}_j\right)
$$
同时在实验中还继续了转导学习和归纳学习
