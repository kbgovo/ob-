---
share: true
---

# [（待完善）SparseGAD：Beyond Homophily Robust Graph Anomaly Detection via Neural Sparsification](<file:///C:\Users\DELL\Desktop\论文\Beyond Homophily Robust Graph Anomaly Detection via Neural Sparsification.pdf>)
## 实现思路
引入一个可学习的邻接矩阵H找出异质节点以及利用稀疏化去除异常节点，这篇文章是在原文[Neural Sparse](<file:///C:\Users\DELL\Desktop\论文\robust graph representation learning via neural sparsification.pdf>)的基础上实现的，稀疏化用的也是原文的方式

## 网络架构
![[sparseGAD网络架构.png|sparseGAD网络架构.png]]

## 实现思路
对于一个图 $G=(V, A, X)$ ，$V$ 表示节点，其中部分节点标注是否是良性节点，而部分节点未知，$A$ 表示邻接矩阵，$X$ 表示每个节点的特征，首先将将节点特征矩阵$X$和邻接矩阵$A$作为输入，先使用线性层变换节点特征，然后利用$k$层的GNN聚合邻居信息并导出每个节点表示：
$$
{\bf h}_{v}^{k}=\sigma\left(\left({\bf h}_{v}^{k-1}+\sum_{u\in\mathcal{N}(v)}c_{u v}^{k}{\bf h}_{u}^{k-1}\right){\bf W}^{k}\right)
$$
其中$W$是可学习参数，$u$是邻居节点，$c$是超参数
为了发现异常和良性节点潜在的邻居，引入通用的homey邻接矩阵$\mathcal{H}$ ,表示如下影响：
![[H矩阵影响.png|H矩阵影响.png]]
而$\mathcal{H}$使用余弦相似度计算得到：
$$
\mathcal{H}_{uv}={\frac{{\bf z}_{u}^{\prime}\cdot{\bf z}_{v}^{\prime}}{\left|{\bf z}_{u}^{\prime}\right|\left|{\bf z}_{v}^{\prime}\right|}}\in\left[-1,1\right]
$$
由于大量良性节点和异常节点连接，过度平滑的邻居信息限制了检测异常节点的能力，因此采用基于*Personalized PageRank-based GNN*的聚合机制，获得节点v的聚合表示：
$$
\mathbf{z}_{v}=\sum_{k=0}^{K}\gamma_{k}\mathbf{h}_{v}^{k}
$$




# [DropAGG: Robust Graph Neural Networks via Drop Aggregation](<file:///C:\Users\DELL\Desktop\论文\dropAGG.pdf>)
## 创新点
提出了一种新的随机消息传播机制，DropAGG的核心是随机选择一定比例的有邻居节点参与分层消息聚合。


## 网络架构
![[DropAGG网络架构.png|DropAGG网络架构.png]]
## 具体实现
对于输入节点，随机采样一定比例节点，只对该部分节点进行聚合操作，实际操作中对每个节点随机采样一个变量（遵循伯努利分布）来判断是非需要聚合；在实验中，使用随机攻击添加一些边或使用Metattack随机增删一些边
# [DropMessage: Unifying Random Dropping for Graph Neural Networks](<file:///C:\Users\DELL\Desktop\论文\DropMessage_ Unifying Random Dropping for Graph Neural Networks.pdf>)
## 创新点
以前的方法是在节点特征矩阵或邻接矩阵上操作，而该方法是对传播的消息执行丢弃操作，也可以视作是丢弃节点、边的统一。随机丢弃可以看做是给目标函数增加了一个额外的正则项，所以模型更加鲁棒了。
## 示意图
![[DropMessage示意图.png|DropMessage示意图.png]]

## 具体实现
设置一个参数对消息矩阵进行丢弃，对每个元素设置伯努利分布判断是否需要丢弃（只在训练过程中执行）。
# [Graph Random Neural Networks for Semi-Supervised Learning on Graphs](<file:///C:\Users\DELL\Desktop\论文\Graph Random Neural Networks for Semi-Supervised Learning on Graphs.pdf>)
## 创新点
提出了一种半监督学习框架GRAND，其中随机传播机制让每个节点的特征可以被部分或者全部丢弃，生成多个图数据增强，之后利用一致性正则化执行预测模型，以提高半监督环境下的模型泛化能力。
## 网络架构

## 具体实现
### 随机传播
随机删除一些节点的整个特征向量，这样能够完全忽略某些节点的特征，仅聚合来自其某条邻居子集的信息，减少了对特定邻居的依赖，从而有助于提高模型的鲁棒性。
### 一致性正则化
一般来说，损失函数=标记节点上的监督损失+图正则化损失。本文中的损失函数=监督损失+一致性正则化损失。其中监督损失只与m个标记节点相关，一致性正则化损失是结果不同数据增强后，结果MLP得到的不同结果。

# [NRGNN: Learning a Label Noise-Resistant Graph Neural Network on Sparsely and Noisily Labeled Graphs](<file:///C:\Users\DELL\Desktop\论文\NRGNN learning a lable noise-resistant graph neural network on sparsely and noisily labeled graphs.pdf>)
## 创新点
将未标记节点与特征相似度高的标记节点连接，并获得更加准确的伪标签节点。基本思想是如果两个节点特征高度相似，那么很有可能具有相同的标签，因此，当标记节点标签正确概率远大于错误标签概率时，将特征高度相似的标记节点连接，可能会带来更多正确的标签信息。
## 网络架构
![[NRGNN网络结构.png|NRGNN网络结构.png]]
## 边预测
使用GNN作为边预测器，这样不仅简单的依赖节点特征，更学习到局部图结构的节点表示。通过GCN获取两点之间的节点表示之后，两者越接近，节点的关联可能性就越大，并设置一个阈值判断是否需要添加边：
$$
{S}_{ij}=ReLU(z_iz_j^T)
$$
之后根据邻接矩阵也可以得到损失函数。

## 伪标签预测
将两个节点之间的权重和节点特征作为输入，利用GNN可以预测节点的伪标签，依旧定义了损失函数，为了提高置信度，预测值必须大于一个阈值才能设置为伪标签。之后再次使用边预测器处理未标记节点和包含伪标签节点和标记节点，添加边，也可以添加损失函数。最终的损失函数是上述提到的3个损失函数加权和


# [RSGNN: Towards robust graph neural networks for noisy graphs with sparse labels](<file:///C:\Users\DELL\Desktop\论文\towards robust graph neural networks for noisy graphs with sparse labels.pdf>)
## 创新点
通过学习去噪致密图开发鲁棒抗噪声GNN，具体来说，采用噪声图中的边作为监督获得去噪致密图，以方便消息传递来预测未标记的节点

## 网络架构
![[RS-GNN架构.png|RS-GNN架构.png]]

## 具体实现
### 边预测
创建一个基于MLP的链接预测器，使用节点特征来预测链接，两个节点之间特征的相似，为其分配的权重就越大。由于给定的图包含结构噪声并缺少边，所以学习一个新图降低噪声边权重同时预测出缺少的边。计算两个节点之间的权重方式如下（先经过MLP学习节点表示）：
$$
{w}_{ij}=ReLU(z_iz_j^T)
$$
对于每个节点来说，随机采样不与其连接的节点作为负样本，在此基础上，需要根据特征相似性重新加权。对于节点及其正样本，通过最小化下面式子来调整 ${w}_{ij}$ ：
$$
\exp\bigl(-\frac{||{\bf x}_{i}-{\bf x}_{j}\,||^{2}}{\sigma^{2}}\bigr)\bigl(w\bigl(i,j\bigr)\,-\,1\bigr)^{2}
$$
其中 $\sigma$ 是控制样本权重的方差。由于节点相似，导致$\exp\bigl(-\frac{||{\bf x}_{i}-{\bf x}_{j}\,||^{2}}{\sigma^{2}}\bigr)$变大，因此$\bigl(w\bigl(i,j\bigr)\,-\,1\bigr)^{2}$需要减小，则权重变大，趋近1

对于节点及其负样本，通过最小化下面式子来调整 ${w}_{ij}$ :
$$
\exp\bigl(\frac{||{\bf x}_{i}-{\bf x}_{n}\,||^{2}}{\sigma^{2}}\bigr)\bigl(w\bigl(i,n\bigr)\,-\,0\bigr)^{2}
$$
由于节点不同，导致 $\exp\bigl(-\frac{||{\bf x}_{i}-{\bf x}_{n}\,||^{2}}{\sigma^{2}}\bigr)$ 变得很大，因此 $\bigl(w\bigl(i,j\bigr)\,-\,1\bigr)^{2}$ 需要减小，故权重趋近0

通过损失函数能够降低噪声边权重并提高缺少边的权重。为了节省计算成本，对于每个节点，先构造一个候选子集，包括原始特征空间中与该节点具有最大余弦相似度的K个节点。通过设置阈值，丢弃一些噪声边和引入更多的边。最终的 **损失函数**=未标记节点平滑度+GNN预测+**边预测**


# [RTGNN：Robust Training of Graph Neural Networks via Noise Governance](<file:///C:\Users\DELL\Desktop\论文\robust training of graph neural networks via noise governance.pdf>)
## 创新点
提出RTGNN，引入自我强化监督纠正噪声标签，从噪声治理的角度增强GNN的鲁棒性，一致性正则化防止过拟合。

## RTGNN网络架构
![[RTGNN网络架构.png|RTGNN网络架构.png]]
## 具体实现
### 前置信息
为了解决节点分类任务中标签节点稀疏且可能错误的问题，GNN会先计算一个logit  $\mathrm{o}_{\theta}^{i}\in{R}^{C}$，该向量中的每个数值表示节点属于每一个类别的概率，节点 $i$ 属于概率 $j$ 计算方式如下：
$$
\mathrm{p}_{\theta}^{i,j}=\exp(\mathrm{o}_{\theta}^{i,j})/\sum_{k=1}^{C}\exp(\mathrm{o}_{\theta}^{i,k})
$$
其中 $\mathrm{o}_{\theta}^{i,k}$ 表示 $\mathrm{o}_{\theta}^{i}$ 的第 $k$ 个元素

### 图增强
使用潜在图学习将输入图致密化，将过程解耦为候选生成和链接推理。对于候选生成，选择每个节点的top-K最近节点（标记节点和未标记节点连接），在节点特征空间中评估距离。之后，使用GCN作为编码器作为边预测器来进行链接推理，节点 $i$ 和 $j$ 之间的预测边权重表示为GCN学习到的节点表示之间的非负余弦相似度：
$$
w_{i j}=\operatorname*{max}(\frac{z^{i}\cdot
z^{j}}{\left|\left|z^{i}\right|\right|\left|\left|z^{j}\right|\right|},0)
$$
基于负采样的重建目标训练边缘预测器：
$$
{{{\mathcal{L}}}}_{rec}=\sum_i\left(\sum_{j:\,\mathbf{A}_{ij}>0}(w_{ij}-\mathbf{A}_{ij})^2+N_{neg}\cdot{E\overline{}}_{j^{\prime}\sim P_n}(w_{ij^{\prime}}-\mathbf{A}_{ij^{\prime}})^2\right)
$$
其中 $N_{neg}$ 是每个节点的负样本数量， $P_{n}$ 是负样本的分布
增强后的图邻接矩阵表示为：
![[图邻接矩阵表示.png|图邻接矩阵表示.png]]
### 噪声治理
#### 干净/噪声节点划分
RTGNN核心在于显示标签噪声治理，因此将增强图输入到一对同结构不同参数的GCN中。首先根据小损失准则*small-loss criterion*将标记节点 $V_{L}$ 划分为干净候选集 $V_{cl}$ 和噪声候选集 $V_{ns}$ .并将交叉熵导出的相互损失定义为节点 $i$ 的损失度量以预测置信度：
$$
{\mathcal{L}}_{mutual}^i=-\mathbf{y}^i\log(\mathbf{p}_{\theta_1}^i)-\mathbf{y}^i\log(\mathbf{p}_{\theta_2}^i)=-\mathbf{y}^i\log(\mathbf{p}_{\theta_1}^i\cdot\mathbf{p}_{\theta_2}^i)
$$
所以干净候选集和噪声候选集的划分依据如下：
![[干净候选集和噪声候选集的划分依据.png|干净候选集和噪声候选集的划分依据.png]]

#### 自我强化监督
干净候选集中的节点由分配的标签监督，噪声候选集中使用自己的预测标签强化训练：首先从噪声候选集中选择一些符合特定条件的节点子集 $V_{sr}$ ，在每个epoch中，更新 $V_{sr}$ 并计算其中每个节点的自适应训练权重

#### 伪标签
从未标记的节点中选择具有置信度和一致性预测的节点生成伪标签。

#### 一致性正则化
对于标记节点和伪标签节点的合集，使用两个对等GCN互相模仿对方的预测，使用DKL散度作为拟态损失，同时又遵循同质性，在同一GCN中模仿邻居的预测，一致性正则化损失由上述两个损失组成。
### 整体训练损失
总损失=标签损失+伪标签损失+$\alpha$ 边预测器损失

# [Pro-GNN: Graph Structure Learning for Robust Graph Neural Networks](<file:///C:\Users\DELL\Desktop\论文\Graph structure learning for robust graph neural networks.pdf>)
## 创新点
探讨了*metattack*攻击对图数据的影响及特性。通过分析发现原始图具有低秩、稀疏、特征平滑的特性，因此从这三个方面探索图结构来重建干净图消除攻击，主要是设计了几个目标损失函数。
## 网络架构
![[Pro-GNN网络架构.png|Pro-GNN网络架构.png]]
## 具体实现
对于噪声邻接矩阵A，通过低秩和稀疏性约束去除对抗边：
![[损失函数L0.png|损失函数L0.png]]
其中S是新的邻接矩阵，应该趋近于A，通过最小化S的L1范数和核范数，以确保稀疏和低秩。最小化核范数的一个重要好处是可以减少每个奇异值，从而减轻攻击扩大奇异值的影响。
对于利用特征平滑度，损失函数可以写成：
![[损失函数LS.png|损失函数LS.png]]
其中，拉普拉斯矩阵L=D-S，D是S的对角矩阵，di表示节点Vi的度，如果i和j相连，那么特征就会相似，则Ls会很小。
总体损失函数为
![[总损失函数.png|总损失函数.png]]
# [IDGL: Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings](<file:///C:\Users\DELL\Desktop\论文\NeurIPS-2020-iterative-deep-graph-learning-for-graph-neural-networks-better-and-robust-node-embeddings-Paper.pdf>)
## 创新点
提出了端到端的图学习框架：迭代深度图学习IDGL，用于联合迭代地学习针对下游预测任务优化的图结构和GNN参数。基于更好的节点嵌入学习更好的图结构，及基于更好的图结构学习更好的节点嵌入学习。基于锚的近似技术，进一步节约时间和内存消耗。
## 网络架构
![[IDGL网络架构.png|IDGL网络架构.png]]


## 具体实现
### 图相似度度量学习（图表示学习）
选择加权余弦相似度作为度量函数
$$
s_{ij}^p=\cos({w}_p\odot{v}_i,{w}_p\odot{v}_j),\;\;\;\;s_{ij}=\frac{1}{m}\sum_{p=1}^ms_{ij}^p
$$
其中 $\odot$ 表示Hadamard积，$w$ 是一个可学习权重向量，维度与 $vi$ 相同，$p$ 表示每个第 $p$ 个视角考虑的向量中的语义信息。为了减小计算资源，使用基于锚的可拓展尺度学习，设计了一个节点-锚亲和矩阵 $R$ ，评估图节点集 $V$ 和锚点集 $U$ ，具体来说，从节点集 $V$ 中随机采样一组点作为锚点 $s$ ，锚点嵌入被设置为相应的节点嵌入，上面的公式可以改写为：
$$
a_{ik}^p=\cos({w}_p\odot{v}_i,{w}_p\odot{v}_j),\;\;\;\;a_{ik}=\frac{1}{m}\sum_{p=1}^ma_{ik}^p
$$
其中是 $a_{ik}$ 节点$vi$和锚点$uk$之间的亲和力得分。
### 图稀疏化
对于每个节点，从S矩阵中提取一个非负对称稀疏矩阵A，设置阈值为$\epsilon$

### 图节点嵌入及预测
将学习到的图与初始图结合起来：
![[Pasted image 20240119201235.png|Pasted image 20240119201235.png]]
最终学习到的图结构是 $A^{(1)}$ 和$A^{(t)}$ 的线性加权和，GNN使用的也是两层

### 节点-锚信息传递
可以通过一系列的矩阵变换从R中获取A

### 图正则化
学习到的图的平滑度、连通性和稀疏度十分重要，因为它反映了与初始节点属性和下游任务相关的图结构